---
title: "Pseudobulk differential analysis on cryptic exons (CEs)"
author: "Mira Sohn"
output:
    html_document:
        code_folding: hide
        df_print: paged
        toc: true
        toc_float: true
        toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE,
                      message=FALSE,
                      cache.lazy=FALSE)
```

Last run: `r date()`

This workflow is designed to conduct pseudobulk differential analysis
using [DESeq2](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-014-0550-8).

Refer to the following resources for technical details:

- [Documentation](https://bioconductor.org/packages/release/bioc/vignettes/DESeq2/inst/doc/DESeq2.html)
- [GitHub](https://github.com/thelovelab/DESeq2)

```{r libraries}
# Libraries
library(tidyverse)
library(DESeq2)
library(ggplot2)
library(parallel)
library(BiocParallel)
library(yaml)
library(RColorBrewer)
library(plotly)
library(parallel)
library(rtracklayer)
library(GenomicRanges)
library(GenomicFeatures)
library(zinbwave)
library(scran)

# Additional options
set.seed(2570)
source('../config/helpers.R')

# Specify the number of available cores
n_cores <- future::availableCores()
options(mc.cores=n_cores)
```

```{r config}

# --------------------------------------------------------------------------------
# This chunk specifies input paths and variables in R
# --------------------------------------------------------------------------------

# Path to input matrices
in_rds <- "sc-exploratory/aggr_matrices.rds"

# Path to Snakemake config
snakemake_config <- "../config/config.yaml"

# Paths to yaml config
config_yaml <- read_yaml(snakemake_config)

# Path to output directory
out_dir <- "sc-pseudobulk-aggr"

# Path to GTF
gtf_path <- "../../../input/thalamus_excitatory/genes.gtf.gz"

# Create directories if missing
for (p in c(out_dir)) {
    if (! dir.exists(p)) { dir.create(p, recursive=TRUE) }
}

# Specify a pseudocount to be added to count matrices
# NOTE: The sparsity of single cell matrix sometimes causes an error
#       that cannot calculate geometric means because of many zeros. 
#       This issue is prevented by adding a small pseudocount.
pseudocount <- 0

# Specify metadata columns for factors and factor levels
factor_groups <- list(
    general_disease=c('Control', 'FTD', 'AD'),
    cell_type=c('ExNeu1', 'ExNeu2'),
    study=c('Marsan', 'Biogen', 'Mathys'),
    study_specific_disease_specific=c('Control-Marsan',
                                      'Control-Biogen',
                                      'Control-Mathys',
                                      'FTD-Marsan',
                                      'FTD-Biogen',
                                      'AD-Mathys'),
    study_disease_specific_celltype=c('Control-Marsan_ExNeu1',
                                      'Control-Marsan_ExNeu2',
                                      'Control-Biogen_ExNeu1',
                                      'Control-Biogen_ExNeu2',
                                      'Control-Mathys_ExNeu1',
                                      'Control-Mathys_ExNeu2',
                                      'FTD-Marsan_ExNeu1',
                                      'FTD-Marsan_ExNeu2',
                                      'FTD-Biogen_ExNeu1',
                                      'FTD-Biogen_ExNeu2',
                                      'AD-Mathys_ExNeu1',
                                      'AD-Mathys_ExNeu2'),
    study_celltype=c('Marsan_ExNeu1', 'Biogen_ExNeu1', 'Mathys_ExNeu1',
                     'Marsan_ExNeu2', 'Biogen_ExNeu2', 'Mathys_ExNeu2')
)

# Specify column names to subset barcodes
# NOTE: Set to NULL if unnecessary
subset_col <- 'study_celltype'
sample_col <- 'SampleID_celltype'
celltype_col <- 'cell_type'
disease_col <- "general_disease"

# Specify DE thresholds
lfc_thresh <- 0
alpha <- 0.1

# Specify CE target genes
ce_targets <- config_yaml[['genes']]

# Aggregate subsets? Set to NULL if unset.
aggregate_subsets <- list(
    MarsanBiogen_ExNeu1=c('Marsan_ExNeu1', 'Biogen_ExNeu1'),
    MarsanBiogen_ExNeu2=c('Marsan_ExNeu2', 'Biogen_ExNeu2')
)
```

# Loading input data {.tabset}

The number of splicing junctions was counted based on cellranger-aligned reads mapped to the following
genes:

```{r genes_tested}
print(ce_targets)
```

Raw junction counts were compiled into a junction-by-barcode matrix for differential testing in the
previous analysis, [`sc-exploratory.html`](sc-exploratory.html). This matrix was preprocessed
to aggregate the counts per sample per celltype, along with their associated metadata table.
These preprocessed matrices and metadata are imported from the following location:

```{r infile_info}
print(in_rds)
```

```{r import_adata}

# --------------------------------------------------------------------------------
# This chunk imports input data saved in the previous single-cell 
# exploratory analysis. Manually edit the following code if necessary.
# --------------------------------------------------------------------------------

# Import input RDS file
obj <- readRDS(in_rds)

# Extract raw count matrices
mat_list <-  map(obj[['bulk']], ~.x[['raw']])

# Aggregate subsets if necessary
if (!is.null(aggregate_subsets)) {
    for (name in names(aggregate_subsets)) {
        # Extract subset names to be aggregated
        subset_i <- aggregate_subsets[[name]]
        # Filter subsets of interest
        subset_list <- mat_list[subset_i] %>% unname()
        # Merge matrices by binding columns
        mat_list[[name]] <- do.call("cbind", subset_list)
    }
    # Remove old subsets
    mat_list <- mat_list[! names(mat_list) %in% unlist(aggregate_subsets)]
}

# Extract the entire sampletable (Edit manually if necessary)
sampletable <- obj[['meta']] %>%
    remove_rownames() %>%
    mutate(SampleID_celltype=paste0(SampleID, "_", cell_type), 
           study_disease_specific_celltype=paste0(study_specific_disease_specific,
                                                  "_",
                                                  cell_type))

# Ensure to have correct levels for each factor columns
for (f in names(factor_groups)) {
    sampletable[[f]] <- factor(sampletable[[f]], levels=factor_groups[[f]])
}


# Prep a list of metadata tables for each subset
coldata_list <- lapply(mat_list, function(m) { 
    # Splice the metadata table based on samples in the matrix
    cdata <- sampletable[sampletable[[sample_col]] %in% colnames(m),
                         c(sample_col, names(factor_groups))] %>%
        unique()
    # Break if columns of the matrix and rows of the metadata don't match
    if (nrow(cdata) != ncol(m)) {
        stop("Rownames of coldata and colnames of matrix should be the same!")
    }
    # Reorder rows of the subsetted metadata table
    rownames(cdata) <- cdata[[sample_col]]
    cdata <- cdata[colnames(m),]
    return(cdata)
})
```

# Subsets {.tabset}

The subset-wise aggregation was performed as summarized below:

```{r subset_overview, results='asis'}

# --------------------------------------------------------------------------------
# This chunk explores the number of samples and metadata info across the subsets
# --------------------------------------------------------------------------------

# Function to summarize the number of samples (N)
summarize_N <- function(meta_df) {

    # Count the number of samples
    n_df <- meta_df %>%
        group_by_at(vars(subset_col, disease_col)) %>%
        summarize(N=n()) %>%
        spread(disease_col, N)
    # Reorder columns
    disease_order <- factor_groups[[disease_col]][factor_groups[[disease_col]] %in% 
                                                  colnames(n_df)]
    n_df <- n_df[, c(subset_col, disease_order)]
    # Replace missing values with zero
    n_df[is.na(n_df)] <- 0
    # Add a new column for all N
    n_df[['all_N']] <- rowSums(n_df[, colnames(n_df)[-1]])

    return(n_df)
}

for (name in names(coldata_list)) {

    df <- coldata_list[[name]] %>%
        remove_rownames()
    cat("##", name, "{.tabset}\n\n")
    cat("### N\n\n")
    # Prep the summary table for N
    n_df <- summarize_N(df)

    # Print tables
    print(knitr::kable(n_df))
    cat('\n\n')

    cat("### Full table\n\n")
    subchunkify(paste0(name, "_subset_N_unfiltered"))
    csv_path <- file.path(out_dir,
                          paste0(name, '_unfiltered_sampletable.csv'))
    write.csv(df,
              csv_path,
              row.names=FALSE,
              quote=FALSE)

    link_table(csv_path)
    cat('\n\n')
}
```

# Pre-filtering nonzero samples {.tabset}

Samples with zero junction counts are removed from each subset. The number of samples before
and after filtering is summarized below.


```{r prefilter_samples, results='asis'}

# --------------------------------------------------------------------------------
# This chunk filters nonzero-count samples
# --------------------------------------------------------------------------------

for (name in names(mat_list)) {
    cat("##", name, "{.tabset}\n\n")
    # Remove zero-count samples from the count matrix
    m <- mat_list[[name]]
    nonzero_samples <- colSums(m) > 0
    m <- m[, nonzero_samples]
    mat_list[[name]] <- m
    # Remove zero-count samples from the metadata table
    cdata_old <- coldata_list[[name]]
    cdata_new <- cdata_old[colnames(m),]
    coldata_list[[name]] <- cdata_new
    
    cat("### N\n\n")
    n_df <- summarize_N(cdata_new)
    cat("A total of",
        sum(!nonzero_samples),
        "samples are removed from the current subset.",
        "Refer to the following table for remaining samples:")
    print(knitr::kable(n_df))
    cat("\n\n")

    cat("### Full table\n\n")
    subchunkify(paste0(name, "_subset_N_filtered"))
    csv_path <- file.path(out_dir,
                          paste0(name, '_filtered_sampletable.csv'))
    write.csv(df,
              csv_path,
              row.names=FALSE,
              quote=FALSE)

    link_table(csv_path)
    cat('\n\n')
}

```

```{r setup_dds, cache=TRUE}

# --------------------------------------------------------------------------------
# This chunk creates dds objects per subset by modeling zero counts with zinbwave
# (https://github.com/mikelove/zinbwave-deseq2/blob/master/zinbwave-deseq2.Rmd)
# --------------------------------------------------------------------------------

# Convert the count matrix into the SummarizedExperiment obj across the subsets
# NOTE: Features that were undetected in any of the samples are removed here.
#       This is required by zinbwave
se_list <- map(mat_list, ~.x[rowSums(.x) > 0,]) %>% 
    map2(coldata_list,
         ~SummarizedExperiment(assays=list(counts=.x), colData=.y))

# Specify a design formula
design_formula <- as.formula(paste("~", disease_col))

# Model zero components
zinb_list <- mclapply(se_list, function(se) {
    nms <- c("counts", setdiff(assayNames(se), "counts"))
    assays(se) <- assays(se)[nms]
    assay(se) <- as.matrix(assay(se))
    zinbwave(Y=se,
             X=design_formula,
             K=0,
             verbose=TRUE,
             observationalWeight=TRUE,
             epsilon=1e12)

    })

# Build a list of dds objects
dds_list <- lapply(zinb_list,
                   DESeqDataSet,
                   design=design_formula) %>%
    # Estimate size factors
    mclapply(estimateSizeFactors, type="poscounts")

# Calculate Scran's sum factors
# NOTE: "Sum factors" are more robust to zeros. This method sums counts across the samples 
#        to eliminate the zeros technically, allowing it to utilize a much larger portion
#        of the features to calculate the scaling factor. In contrast, the size factors
#        estimated by the `estimateSizeFactors` function from DESeq2 relies on
#        geometric mean. This value becomes zero if a sample has a count of zero 
#        for any feature.
scr_list <- map(dds_list, computeSumFactors)

# Replace size factors with Scran's sum factors
dds_list <- mclapply(
    names(dds_list),
    function(name) {
        sizeFactors(dds_list[[name]]) <- sizeFactors(scr_list[[name]])
        dds_list[[name]] }) %>% 
    # Run DESeq2
    # lapply(DESeq,
           # test="LRT",
           # reduced=~1,
           # minmu=1e-6,
           # minRep=Inf,
           # parallel=TRUE) %>%
    lapply(function(dds) {
        dds <- estimateDispersionsGeneEst(dds)
        dispersions(dds) <- mcols(dds)$dispGeneEst
        dds <- nbinomLRT(dds, reduced=~1, minmu=1e-6)
        return(dds)
}) %>%
    set_names(names(dds_list))


# Run log2(normalized_counts + 1) transformation
# NOTE: The current dataset cannot use the `varianceStabilizingTransformation` nor `vst` function as 
#       the gene-wise dispersions are almost constant over the mean, so DESeq2 cannot fit 
#       its usual mean–dispersion curve (“parametric”, “local”, or “mean”).
l2t_list <- lapply(names(dds_list), function(name) {
    dds <- dds_list[[name]]
    # varianceStabilizingTransformation(dds, blind=TRUE)
    m <- log2(counts(dds, normalized=TRUE) + 1)
    return(m)
}) %>%
set_names(names(dds_list))
```

# Quality control (QC)

## Sample similarity heatmap {.tabset}

Sample-to-sample similarity in junction profiling is explored using a heatmap of normalized counts 
(log2 scale), with *Euclidean distance* represented by the color scale. Darker blue indicates
less distance (i.e., greater similarity) in splice junction profiling.

```{r qc_heatmap, results='asis', fig.height=8, fig.width=8}

# --------------------------------------------------------------------------------
# This chunk explores sample similarity using heatmap
# --------------------------------------------------------------------------------

for (name in names(l2t_list)) {
    cat('###', name, '{.tabset}\n\n')
    # Prep a matrix
    scmat <- l2t_list[[name]] %>%
        t() %>%
        dist() %>%
        as.matrix()

    # Set color to be displayed
    colors <- colorRampPalette(rev(RColorBrewer::brewer.pal(9, 'Blues')))(255)

    # Prep metadata for heatmap
    h_meta <- coldata_list[[name]]
    h_meta <- h_meta[, c(disease_col, subset_col)]
    h_meta <- h_meta[, colnames(h_meta) != sample_col]

    # Print heatmap
    print(pheatmap::pheatmap(
        scmat,
        scale='none',
        color=colors,
        annotation_col=h_meta,
        show_rownames=FALSE,
        show_colnames=FALSE,
        cluster_cols=TRUE,
        border_color=NA
    )
    )
    cat('\n\n')
}

```

## PCA {.tabset}

Another way of looking at sample clustering is principal components analysis (PCA). Each axis does
not have units, rather, it represents the dimensions along which the samples vary the most. The amount 
of variance explained by each principal component is indicated in the axes label.

```{r qc_pca, results='asis'}

# --------------------------------------------------------------------------------
# This chunk explores sample similarity using PCA
# --------------------------------------------------------------------------------

for (name in names(l2t_list)) {
    cat('###', name, '{.tabset}\n\n')
    # Prep a matrix
    scmat <- l2t_list[[name]] %>%
        t()

    # Prep metadata
    p_meta <- coldata_list[[name]]

    # Run PCA
    pca <- prcomp(scmat, center=TRUE, scale=FALSE)
    # Calculate variance explained by each PC
    var_exp <- round(pca$sdev^2/sum(pca$sdev^2) * 100, 2)

    # Prep input dataframe for plotting
    df <- pca$x[,1:2] %>%
        as.data.frame() %>%
        rownames_to_column(sample_col) %>%
        left_join(p_meta, by=sample_col)
    # Prep titles for axes
    axis_titles <- map_chr(
        1:2,
        ~paste0("PC ", .x, " (", var_exp[.x], "%)")
    )

    # Plot PCA
    condition_cols <- colnames(p_meta)[-1] %>%
        sapply(function(c) length(unique(p_meta[[c]])) > 1)
    condition_cols <- colnames(p_meta)[-1][condition_cols]
    for (c in condition_cols) {
        cat('####', c, '\n\n')
        p <- ggplot(df,
                    aes_string(x='PC1',
                               y='PC2',
                               color=c,
                               shape=celltype_col)) +
            geom_point(alpha=0.7, size=3) +
            theme_bw() +
            xlab(axis_titles[1]) +
            ylab(axis_titles[2])

        print(p)
        cat('\n\n')
    }
}

```

## Size factors {.tabset}

Ideally, all libraries were sequenced to identical depth, in which case all size factors would be 1.0. 
In practice, this is almost never the case due to the difficulties of accurately measuring 
low concentrations of cDNA. DESeq2 uses size factor estimates to normalized for sequencing depth across 
libraries. If some libraries are much higher or lower than 1 then those libraries had dramatically
different coverage and we should be careful about interpreting results.


Unlike the standard DESeq2 workflow, which uses the median-of-ratio method, the current analysis
estimates size factors using the [deconvolution method](https://doi.org/10.1186/s13059-016-0947-7).
This method is robust to many zeros by pooling counts across samples during size factor estimation,
whereas the median-of-ratio method relies on geometric means, which returns zero if any feature
has a count of zero.

These diagnostic plots show the size factors (as a ranked bar plot) and the relationship between the size 
factors and the total read count (as a scatterplot). Samples whose total read count differs from size
factor may indicate that the sample has a small number of highly expressed features.

```{r size_factors, results='asis'}

# --------------------------------------------------------------------------------
# This chunk explores size factors across the samples
# --------------------------------------------------------------------------------

for (name in names(dds_list)) {
    cat('###', name, '{.tabset}\n\n')
    # Extract and reorder size factors
    dds <- dds_list[[name]]
    sf <- sizeFactors(dds)
    sf <- sf[order(sf)] %>%
        tibble::enframe(value="Size Factor")

    p <- ggplot(sf) +
        aes(x=reorder(name, `Size Factor`), y=`Size Factor`) +
        xlab('cluster') +
        geom_col() +
        theme_bw() +
        theme(axis.text.x=element_text(angle=90, vjust=0.5, hjust=1))

    cat('#### Size factors\n\n')
    subchunkify(paste(name, '_sizefactor1'), input='plot', width=10)
    cat('\n\n')

    trc <- colSums(counts(dds)) %>% 
        tibble::enframe(value = 'Total Read Count')
    trc_vs_sf <- dplyr::full_join(sf, trc, by='name')
    p <- ggplot(data=trc_vs_sf,
                aes_string(x="`Total Read Count`", y="`Size Factor`", label='name')) +
        geom_point(size=3) +
        theme_bw() +
        theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

    cat('#### Size factors vs total read counts\n\n')
    subchunkify(paste(name, '_sizefactor2'), input='plot')
    cat('\n\n')
}

```

## Dispersion {.tabset}

The DESeq2 dispersion estimates are inversely related to the mean and directly related to variance.
Based on this relationship, the dispersion is higher for small mean counts and lower for large 
mean counts. Therefore, the dispersion estimates reflect the variance in gene expression for a given 
mean value. 

DESeq2 shares information across genes to generate more accurate estimates of variation based on 
the mean expression level of the gene using a method called ‘shrinkage’. DESeq2 assumes that genes 
with similar expression levels have similar dispersion.


```{r dispersion, results='asis'}

# --------------------------------------------------------------------------------
# This chunk explores dispersions across the features
# --------------------------------------------------------------------------------

for (name in names(dds_list)) {

    cat('###', name, '\n\n')
    DESeq2::plotDispEsts(dds_list[[name]])
    cat('\n\n')
}

```



# Session info

```{r session_info, collapse=FALSE}
sessionInfo()
```

