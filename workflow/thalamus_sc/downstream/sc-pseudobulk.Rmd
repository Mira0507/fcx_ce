---
title: "Pseudobulk differential analysis on cryptic exons (CEs)"
author: "Mira Sohn"
output:
    html_document:
        code_folding: hide
        df_print: paged
        toc: true
        toc_float: true
        toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE,
                      message=FALSE,
                      cache.lazy=FALSE)
```

Last run: `r date()`

This workflow is designed to conduct pseudobulk differential analysis
using [DESeq2](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-014-0550-8).

Refer to the following resources for technical details:

- [Documentation](https://bioconductor.org/packages/release/bioc/vignettes/DESeq2/inst/doc/DESeq2.html)
- [GitHub](https://github.com/thelovelab/DESeq2)

```{r libraries}
# Libraries
library(tidyverse)
library(DESeq2)
library(ggplot2)
library(parallel)
library(yaml)
library(RColorBrewer)
library(plotly)
library(parallel)
library(rtracklayer)
library(GenomicRanges)
library(GenomicFeatures)

# Additional options
set.seed(2570)
source('../config/helpers.R')

# Specify the number of available cores
n_cores <- future::availableCores()
options(mc.cores=n_cores)
```

```{r config}

# --------------------------------------------------------------------------------
# This chunk specifies input paths and variables in R
# --------------------------------------------------------------------------------

# Path to input matrices
in_rds <- "sc-exploratory/aggr_matrices.rds"

# Path to Snakemake config
snakemake_config <- "../config/config.yaml"

# Paths to yaml config
config_yaml <- read_yaml(snakemake_config)

# Path to output directory
out_dir <- "sc-pseudobulk"

# Path to GTF
gtf_path <- "../../../input/thalamus_excitatory/genes.gtf.gz"

# Create directories if missing
for (p in c(out_dir)) {
    if (! dir.exists(p)) { dir.create(p, recursive=TRUE) }
}

# Specify a pseudocount to be added to count matrices
# NOTE: The sparsity of single cell matrix sometimes causes an error
#       that cannot calculate geometric means because of many zeros. 
#       This issue is prevented by adding a small pseudocount.
pseudocount <- 1

# Specify metadata columns for factors and factor levels
factor_groups <- list(
    general_disease=c('Control', 'FTD', 'AD'),
    cell_type=c('ExNeu1', 'ExNeu2'),
    study=c('Marsan', 'Biogen', 'Mathys'),
    study_specific_disease_specific=c('Control-Marsan',
                                      'Control-Biogen',
                                      'Control-Mathys',
                                      'FTD-Marsan',
                                      'FTD-Biogen',
                                      'AD-Mathys'),
    study_disease_specific_celltype=c('Control-Marsan_ExNeu1',
                                      'Control-Marsan_ExNeu2',
                                      'Control-Biogen_ExNeu1',
                                      'Control-Biogen_ExNeu2',
                                      'Control-Mathys_ExNeu1',
                                      'Control-Mathys_ExNeu2',
                                      'FTD-Marsan_ExNeu1',
                                      'FTD-Marsan_ExNeu2',
                                      'FTD-Biogen_ExNeu1',
                                      'FTD-Biogen_ExNeu2',
                                      'AD-Mathys_ExNeu1',
                                      'AD-Mathys_ExNeu2'),
    study_celltype=c('Marsan_ExNeu1', 'Biogen_ExNeu1', 'Mathys_ExNeu1',
                     'Marsan_ExNeu2', 'Biogen_ExNeu2', 'Mathys_ExNeu2')
)

# Specify column names to subset barcodes
# NOTE: Set to NULL if unnecessary
subset_col <- 'study_celltype'
sample_col <- 'SampleID_celltype'
celltype_col <- 'cell_type'
disease_col <- "general_disease"

# Specify DE thresholds
lfc_thresh <- 0
alpha <- 0.1
```

# Loading input data {.tabset}

The number of splicing junctions was counted based on cellranger-aligned reads mapped to the following
genes:

```{r genes_tested}
print(config_yaml[['genes']])
```

Raw junction counts were compiled into a junction-by-barcode matrix for differential testing in the
previous analysis, [`sc-exploratory.html`](sc-exploratory.html). This matrix was preprocessed
to aggregate the counts per sample per celltype, along with their associated metadata table.
These preprocessed matrices and metadata are imported from the following location:

```{r infile_info}
print(in_rds)
```

```{r import_adata}

# --------------------------------------------------------------------------------
# This chunk imports input data saved in the previous single-cell 
# exploratory analysis. Manually edit the following code if necessary.
# --------------------------------------------------------------------------------

# Import input RDS file
obj <- readRDS(in_rds)

# Extract count matrices
mat_list <- obj[['bulk']]

# Extract the entire sampletable (Edit manually if necessary)
sampletable <- obj[['meta']] %>%
    remove_rownames() %>%
    mutate(SampleID_celltype=paste0(SampleID, "_", cell_type), 
           study_disease_specific_celltype=paste0(study_specific_disease_specific,
                                                  "_",
                                                  cell_type))

# Ensure to have correct levels for each factor columns
for (f in names(factor_groups)) {
    sampletable[[f]] <- factor(sampletable[[f]], levels=factor_groups[[f]])
}


# Prep a list of metadata tables for each subset
coldata_list <- mclapply(mat_list, function(m) { 
    # Splice the metadata table based on samples in the matrix
    cdata <- sampletable[sampletable[[sample_col]] %in% colnames(m),
                         c(sample_col, names(factor_groups))] %>%
        unique()
    # Break if columns of the matrix and rows of the metadata don't match
    if (nrow(cdata) != ncol(m)) {
        stop("Rownames of coldata and colnames of matrix should be the same!")
    }
    # Reorder rows of the subsetted metadata table
    rownames(cdata) <- cdata[[sample_col]]
    cdata <- cdata[colnames(m),]
    return(cdata)
})
```

# Subsets {.tabset}

The subset-wise aggregation was performed as summarized below:

```{r subset_overview, results='asis'}

# --------------------------------------------------------------------------------
# This chunk explores the number of samples and metadata info across the subsets
# --------------------------------------------------------------------------------

# Function to summarize the number of samples (N)
summarize_N <- function(meta_df) {

    # Count the number of samples
    n_df <- meta_df %>%
        group_by_at(vars(subset_col, disease_col)) %>%
        summarize(N=n()) %>%
        spread(disease_col, N)
    # Reorder columns
    disease_order <- factor_groups[[disease_col]][factor_groups[[disease_col]] %in% 
                                                  colnames(n_df)]
    n_df <- n_df[, c(subset_col, disease_order)]
    # Replace missing values with zero
    n_df[is.na(n_df)] <- 0
    # Add a new column for all N
    n_df[['all_N']] <- rowSums(n_df[, colnames(n_df)[-1]])

    return(n_df)
}

for (name in names(coldata_list)) {

    df <- coldata_list[[name]] %>%
        remove_rownames()
    cat("##", name, "{.tabset}\n\n")
    cat("### N\n\n")
    # Prep the summary table for N
    n_df <- summarize_N(df)

    # Print tables
    print(knitr::kable(n_df))
    cat('\n\n')

    cat("### Full table\n\n")
    subchunkify(paste0(name, "_subset_N_unfiltered"))
    csv_path <- file.path(out_dir,
                          paste0(name, '_unfiltered_sampletable.csv'))
    write.csv(df,
              csv_path,
              row.names=FALSE,
              quote=FALSE)

    link_table(csv_path)
    cat('\n\n')
}
```

# Pre-filtering nonzero samples {.tabset}

Samples with zero junction counts are removed from each subset. The number of samples before
and after filtering is summarized below.


```{r prefilter_samples, results='asis'}

# --------------------------------------------------------------------------------
# This chunk filters nonzero-count samples
# --------------------------------------------------------------------------------

for (name in names(mat_list)) {
    cat("##", name, "{.tabset}\n\n")
    # Remove zero-count samples from the count matrix
    m <- mat_list[[name]]
    nonzero_samples <- colSums(m) > 0
    m <- m[, nonzero_samples]
    mat_list[[name]] <- m
    # Remove zero-count samples from the metadata table
    cdata_old <- coldata_list[[name]]
    cdata_new <- cdata_old[colnames(m),]
    coldata_list[[name]] <- cdata_new
    
    cat("### N\n\n")
    n_df <- summarize_N(cdata_new)
    cat("A total of",
        sum(!nonzero_samples),
        "samples are removed from the current subset.",
        "Refer to the following table for remaining samples:")
    print(knitr::kable(n_df))
    cat("\n\n")

    cat("### Full table\n\n")
    subchunkify(paste0(name, "_subset_N_filtered"))
    csv_path <- file.path(out_dir,
                          paste0(name, '_filtered_sampletable.csv'))
    write.csv(df,
              csv_path,
              row.names=FALSE,
              quote=FALSE)

    link_table(csv_path)
    cat('\n\n')
}

```


```{r setup_dds, cache=TRUE}

# --------------------------------------------------------------------------------
# This chunk creates dds objects per subset
# --------------------------------------------------------------------------------

# Build a list of dds objects after adding a pseudocount of 1 across the subsets
# NOTE: While DESeq2 discourages manipulating raw counts, the sparsity (many zeros) 
#       of the count matrices ended up raising errors in estimating size factors and 
#       dispersions. The pseudocount is added to prevent such errors.
#       e.g. 
#       Error in estimateSizeFactorsForMatrix(counts(object), locfunc = locfunc,  : 
#       every gene contains at least one zero, cannot compute log geometric means
dds_list <- mclapply(names(coldata_list), function(name) {
    DESeqDataSetFromMatrix(mat_list[[name]] + pseudocount,
                           colData=coldata_list[[name]],
                           design=~general_disease + 0)
}) %>%
# Run DESeq
# NOTE: The current dataset cannot run the DESeq function due to the following error;
#      Error in estimateDispersionsFit(object, fitType = fitType) :
#      all gene-wise dispersion estimates are within 2 orders of magnitude
#      from the minimum value, and so the standard curve fitting techniques will not work.
#      One can instead use the gene-wise estimates as final estimates:
#      dds <- estimateDispersionsGeneEst(dds)
#      dispersions(dds) <- mcols(dds)$dispGeneEst
#      ...then continue with testing using nbinomWaldTest or nbinomLRT
mclapply(function(dds) {
    # dds <- DESeq(dds, parallel=TRUE, fitType="mean")
    dds <- estimateSizeFactors(dds) %>%
        estimateDispersionsGeneEst()
    dispersions(dds) <- mcols(dds)$dispGeneEst
    dds <- nbinomWaldTest(dds)
    return(dds)
}) %>%
set_names(names(coldata_list))

# Run log2(normalized_counts + 1) transformation
# NOTE: The current dataset cannot use the `varianceStabilizingTransformation` nor `vst` function as 
#       the gene-wise dispersions are almost constant over the mean, so DESeq2 cannot fit 
#       its usual mean–dispersion curve (“parametric”, “local”, or “mean”).
l2t_list <- lapply(names(dds_list), function(name) {
    dds <- dds_list[[name]]
    # varianceStabilizingTransformation(dds, blind=TRUE)
    m <- log2(counts(dds, normalized=TRUE) + 1)
    return(m)
}) %>%
set_names(names(dds_list))
```

# Quality control (QC)

## Sample similarity heatmap {.tabset}

Sample-to-sample similarity in junction profiling is explored using a heatmap of normalized counts 
(log2 scale), with *Euclidean distance* represented by the color scale. Darker blue indicates
less distance (i.e., greater similarity) in splice junction profiling.

```{r qc_heatmap, results='asis', fig.height=8, fig.width=8}

# --------------------------------------------------------------------------------
# This chunk explores sample similarity using heatmap
# --------------------------------------------------------------------------------

for (name in names(l2t_list)) {
    cat('###', name, '{.tabset}\n\n')
    # Prep a matrix
    scmat <- l2t_list[[name]] %>%
        t() %>%
        dist() %>%
        as.matrix()

    # Set color to be displayed
    colors <- colorRampPalette(rev(RColorBrewer::brewer.pal(9, 'Blues')))(255)

    # Prep metadata for heatmap
    h_meta <- coldata_list[[name]]
    h_meta <- h_meta[, c(disease_col, subset_col)]
    h_meta <- h_meta[, colnames(h_meta) != sample_col]

    # Print heatmap
    print(pheatmap::pheatmap(
        scmat,
        scale='none',
        color=colors,
        annotation_col=h_meta,
        show_rownames=FALSE,
        show_colnames=FALSE,
        cluster_cols=TRUE,
        border_color=NA
    )
    )
    cat('\n\n')
}

```

## PCA {.tabset}

Another way of looking at sample clustering is principal components analysis (PCA). Each axis does
not have units, rather, it represents the dimensions along which the samples vary the most. The amount 
of variance explained by each principal component is indicated in the axes label.

```{r qc_pca, results='asis'}

# --------------------------------------------------------------------------------
# This chunk explores sample similarity using PCA
# --------------------------------------------------------------------------------

for (name in names(l2t_list)) {
    cat('###', name, '{.tabset}\n\n')
    # Prep a matrix
    scmat <- l2t_list[[name]] %>%
        t()

    # Prep metadata
    p_meta <- coldata_list[[name]]

    # Run PCA
    pca <- prcomp(scmat, center=TRUE, scale=FALSE)
    # Calculate variance explained by each PC
    var_exp <- round(pca$sdev^2/sum(pca$sdev^2) * 100, 2)

    # Prep input dataframe for plotting
    df <- pca$x[,1:2] %>%
        as.data.frame() %>%
        rownames_to_column(sample_col) %>%
        left_join(p_meta, by=sample_col)
    # Prep titles for axes
    axis_titles <- map_chr(
        1:2,
        ~paste0("PC ", .x, " (", var_exp[.x], "%)")
    )

    # Plot PCA
    condition_cols <- colnames(p_meta)[-1] %>%
        sapply(function(c) length(unique(p_meta[[c]])) > 1)
    condition_cols <- colnames(p_meta)[-1][condition_cols]
    for (c in condition_cols) {
        cat('####', c, '\n\n')
        p <- ggplot(df,
                    aes_string(x='PC1',
                               y='PC2',
                               color=c,
                               shape=celltype_col)) +
            geom_point(alpha=0.7, size=3) +
            theme_bw() +
            xlab(axis_titles[1]) +
            ylab(axis_titles[2])

        subchunkify(paste0("PCA_", name, "_", c), input="plot")
        cat('\n\n')
    }
}

```

## Size factors {.tabset}

Ideally, all libraries were sequenced to identical depth, in which case all size factors would be 1.0. 
In practice, this is almost never the case due to the difficulties of accurately measuring 
low concentrations of cDNA. DESeq2 uses size factor estimates to normalized for sequencing depth across 
libraries. If some libraries are much higher or lower than 1 then those libraries had dramatically
different coverage and we should be careful about interpreting results.

Simply taking the total number of reads has been shown to be too sensitive to a small number of 
highly-expressed features. DESeq2’s size factors are calculated according to the median-ratio method 
(equation 5 of Anders & Huber 2010).

These diagnostic plots show the size factors (as a ranked bar plot) and the relationship between the size 
factors and the total read count (as a scatterplot). Samples whose total read count differs from size
factor may indicate that the sample has a small number of highly expressed features.

```{r size_factors, results='asis'}

# --------------------------------------------------------------------------------
# This chunk explores size factors across the samples
# --------------------------------------------------------------------------------

for (name in names(dds_list)) {
    cat('###', name, '{.tabset}\n\n')
    # Extract and reorder size factors
    dds <- estimateSizeFactors(dds_list[[name]])
    sf <- sizeFactors(dds)
    sf <- sf[order(sf)] %>%
        tibble::enframe(value="Size Factor")

    p <- ggplot(sf) +
        aes(x=reorder(name, `Size Factor`), y=`Size Factor`) +
        xlab('cluster') +
        geom_col() +
        theme_bw() +
        theme(axis.text.x=element_text(angle=90, vjust=0.5, hjust=1))

    cat('#### Size factors\n\n')
    subchunkify(paste(name, '_sizefactor1'), input='plot', width=10)
    cat('\n\n')

    trc <- colSums(counts(dds)) %>% 
        tibble::enframe(value = 'Total Read Count')
    trc_vs_sf <- dplyr::full_join(sf, trc, by='name')
    p <- ggplot(data=trc_vs_sf,
                aes_string(x="`Total Read Count`", y="`Size Factor`", label='name')) +
        geom_point(size=3) +
        theme_bw() +
        theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

    cat('#### Size factors vs total read counts\n\n')
    subchunkify(paste(name, '_sizefactor2'), input='plot')
    cat('\n\n')
}

```

## Dispersion {.tabset}

The DESeq2 dispersion estimates are inversely related to the mean and directly related to variance.
Based on this relationship, the dispersion is higher for small mean counts and lower for large 
mean counts. Therefore, the dispersion estimates reflect the variance in gene expression for a given 
mean value. 

DESeq2 shares information across genes to generate more accurate estimates of variation based on 
the mean expression level of the gene using a method called ‘shrinkage’. DESeq2 assumes that genes 
with similar expression levels have similar dispersion.


```{r dispersion, results='asis'}

# --------------------------------------------------------------------------------
# This chunk explores dispersions across the features
# --------------------------------------------------------------------------------

for (name in names(dds_list)) {

    cat('###', name, '\n\n')
    DESeq2::plotDispEsts(dds_list[[name]])
    cat('\n\n')
}

```

# Differential expression {.tabset}

We perform pairwise differential testing using the [Wald test](https://en.wikipedia.org/wiki/Wald_test);
therefore, subsets containing more than one disease status are not examined.


## Summary table

- **subset**: study and celltype
- **nonzero.vs.total**: the number of junctions with nonzero read counts and the total number of 
  tested junctions
- **alpha**: false discovery rate (FDR) cutoff determining significant genes
- **lfcThreshold**: by default, the null hypothesis is that the log2 fold change of splicing is 
  not different from zero. In some circumstances, it is useful to use a different threshold, 
  which will be reported here.
- **outliers**: Cook’s distance is used as a measure of how much a single sample is influencing 
  the fitted coefficients for a junction. If that value is too high, the gene is marked as an 
  outlier and the pvalue and adjusted pvalue will be set to NA. If there are many 
  (hundreds to thousands) of outliers, this is an indication that a sample may be problematic. 
  In this case, the dds diagnostics plots may help identify the culprit.
- **low.counts**: How many junctions were not even tested for differential analysis because they 
  had too low counts.
- **test**: The contrast performed using the design. `A vs B` indicates differential expression of junctions
  in A (experimental) compared to B (control).


```{r de_summary, results='asis'}

# --------------------------------------------------------------------------------
# This chunk summarizes testing results
# --------------------------------------------------------------------------------

# Calculate results
res_list <- mclapply(dds_list[!str_detect(names(dds_list), "all")], results)

lapply(names(res_list), function(name) {
    # Extract res and dds objects
    res <- res_list[[name]]
    dds <- dds_list[[name]]
    # Summary metrics
    notallzero <- sum(res$baseMean > 0)
    up <- sum(res$padj < alpha & res$log2FoldChange > lfc_thresh, na.rm=TRUE)
    down <- sum(res$padj < alpha & res$log2FoldChange < -lfc_thresh, na.rm=TRUE)
    filt <- sum(!is.na(res$pvalue) & is.na(res$padj))
    outlier <- sum(res$baseMean > 0 & is.na(res$pvalue))
    test <- mcols(res)['log2FoldChange', 'description']

    # Build a summary dataframe
    data.frame(
        subset=name,
        up=up,
        down=down,
        nonzero.vs.total=paste0(notallzero, '/', nrow(res)), 
        alpha=alpha,
        lfcThreshold=lfc_thresh,
        outliers=outlier,
        low.counts=filt,
        # adjust width.cutoff here because newline insertion causes this to return
        # a df with multiple duplicate rows
        design=deparse(design(dds), width.cutoff=500L),
        test=test
    )
}) %>%
bind_rows() %>%
knitr::kable()
```

## Full table {.tabset}

Metrics:

- **chr**, **start**, **end**: junction genomic coordinates
- **gene_name**: gene name corresponding to the splice junction
- **baseMean**: mean normalized counts of the splice junction in all samples
- **log2FoldChange**: fold changes in a log2 scale
- **lfcSE**: standard error of the log2 fold change. A smaller value suggests a more precise estimate of 
  the fold change.
- **stat**: test statistic. For the Wald test, this metric refers to the log2FoldChange divided by lfcSE,
  which is compared to a standard normal distribution to generate a two-tailed pvalue. For the likelihood
  ratio test (LRT), stat is the difference in deviance between the reduced model and the full model, which 
  is compared to a chi-squared distribution to generate a pvalue.
- **pvalue**: raw p-values
- **padj**: false discovery rate (FDR) computed using the Benjamini-Hochberg (BH) method
- **exon_type**:
    - *cryptic*: one or both of the start and end coordinates of the junction is within intron regions
    - *non-cryptic-annotated*: both the start and end of the junction coordinates are at annotated positions
    - *non-cryptic-novel*: neither coordinate of the junction is within an intron AND either position 
      is at non-annotated positions

It's assumed that only the *cryptic* type generates CEs by starting or ending at an intron.

```{r de_full, results='asis'}

# --------------------------------------------------------------------------------
# This chunk displays full results
# --------------------------------------------------------------------------------

for (name in names(res_list)) {
    cat('###', name, '\n\n')
    # Clean the result data frame
    df <- res_list[[name]] %>%
        as.data.frame() %>%
        rownames_to_column('exon_type') %>%
        separate(exon_type,
                 c('chr', 'start', 'end', 'gene_name', 'exon_type'),
                 sep=":") %>%
        mutate(exon_detection=ifelse(exon_type == "cryptic",
                                     "CE detected",
                                     "junction detected without CE")) %>%
        remove_rownames()

    # Print
    subchunkify(paste0(name, '_differential_testing'))
    cat('\n\n')

    # Save
    csv_path <- file.path(out_dir,
                          paste0(name, '_differential_results.csv'))
    write.csv(df, csv_path, row.names=FALSE, quote=FALSE)
    link_table(csv_path)
    cat('\n\n')
}
```

# Counts for CEs

```{r norm_ce, results='asis'}

```


# BED files {.tabset}

## Splice junctions

The following BED file contains the splice junctions tested in the current differential analysis.
Refer to the naming convention when loading the file in IGV: `<chr>:<start>:<end>:<gene_name>:<exon_type>`. 

```{r create_intron_bed, results='asis'}

# --------------------------------------------------------------------------------
# This chunk preps a BED file for tested splice junctions
# --------------------------------------------------------------------------------

# Prep coordinates and annotation in a data frame
df <- data.frame(junction=rownames(dds_list[[1]])) %>%
    separate(junction,
             c('chr', 'start', 'end', 'gene_name', 'exon_type'),
             sep=":") %>%
    unite("annotation", chr:exon_type, remove=FALSE, sep=":") %>%
    dplyr::select(chr, start, end, annotation)

# Save
bed_path <- file.path(out_dir, "junctions.bed")
write.table(df,
            file=bed_path,
            sep="\t",
            col.names=FALSE,
            row.names=FALSE,
            quote=FALSE)

link_table(bed_path)
```
 
## Exons

Filtered exons for genes of interest are saved as a BED file for data exploration in IGV.


```{r create_exon_bed, results='asis', cache=TRUE}

# --------------------------------------------------------------------------------
# This chunk preps a BED file for all exons in the GTF file
# --------------------------------------------------------------------------------

# Import GTF annotation
gtf <- import(gtf_path)

# Filter genes of interest and exons
gtf <- gtf[gtf$type == "exon" & mcols(gtf)$gene_name %in% 
           config_yaml[['genes']]]

# Prep a data frame having a BED format for the filtered coordinates
df <- data.frame(
    seqnames = as.character(seqnames(gtf)),
    start = start(gtf) - 1,
    end = end(gtf),
    name = mcols(gtf)$gene_name,
    score = ".",
    strand = as.character(strand(gtf))
    )

# Break of any missing values are found
if (any(is.na(df))) {
    stop("BED file cannot include missing values!")
}

# Save as a BED file
bed_path <- file.path(out_dir, "exons.bed")
write.table(df,
            bed_path,
            col.names=FALSE,
            row.names=FALSE,
            quote=FALSE,
            sep="\t")

link_table(bed_path)
```

# Session info

```{r session_info, collapse=FALSE}
sessionInfo()
```

