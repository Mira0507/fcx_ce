---
title: "Single cell exploratory analysis on cryptic exons"
author: "Mira Sohn"
output:
    html_document:
        code_folding: hide
        df_print: paged
        toc: true
        toc_float: true
        toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE,
                      message=FALSE,
                      cache.lazy=FALSE)
```

Last run: `r date()`

This workflow is designed for exploratory analysis cryptic exons detected at the single-cell level.


```{r libraries}
# Libraries
library(parallel)
library(tidyverse)
library(ggplot2)
library(reticulate)
library(parallel)
library(yaml)
library(reshape2)
library(limma)
library(NMF)
library(MAST)
library(RColorBrewer)
library(scater)
library(plotly)

# Additional options
set.seed(2570)
source('../config/helpers.R')
use_condaenv("../../../menv")

# Specify the number of available cores
n_cores <- future::availableCores()
options(mc.cores=n_cores)

pd <- import("pandas")
```

```{r config}

# --------------------------------------------------------------------------------
# This chunk specifies input paths and variables in R
# --------------------------------------------------------------------------------

# Path to input matrix
mtx_path <- "../results/junction_counts/thalamus_sc_perind_numers.counts.gz"

# Path to metadata table
meta_path <- "../results/barcodes.tsv"

# Path to Snakemake config
snakemake_config <- "../config/config.yaml"

# Paths to AnnData
adata_paths <- read_yaml(snakemake_config)[['adata']] %>%
    map(~paste0("../", .x))

# Path to output directory
out_dir <- "sc-exploratory"

# Create directories if missing
for (p in c(out_dir)) {
    if (! dir.exists(p)) { dir.create(p, recursive=TRUE) }
}

# Specify metadata columns for factors and factor levels
factor_groups <- list(
    general_disease=c('Control', 'FTD', 'AD'),
    cell_type=c('ExNeu1', 'ExNeu2'),
    study=c('Marsan', 'Biogen', 'Mathys'),
    study_specific_disease_specific=c('Control-Marsan',
                                      'Control-Biogen',
                                      'Control-Mathys',
                                      'FTD-Marsan',
                                      'FTD-Biogen',
                                      'AD-Mathys'),
    study_disease_specific_celltype=c('Control-Marsan_ExNeu1',
                                      'Control-Marsan_ExNeu2',
                                      'Control-Biogen_ExNeu1',
                                      'Control-Biogen_ExNeu2',
                                      'Control-Mathys_ExNeu1',
                                      'Control-Mathys_ExNeu2',
                                      'FTD-Marsan_ExNeu1',
                                      'FTD-Marsan_ExNeu2',
                                      'FTD-Biogen_ExNeu1',
                                      'FTD-Biogen_ExNeu2',
                                      'AD-Mathys_ExNeu1',
                                      'AD-Mathys_ExNeu2')
)

# Specify column names to subset barcodes
# NOTE: Set to NULL if unnecessary
subset_col <- list(study_celltype=c('study', 'cell_type'))
sample_col <- 'SampleID'
celltype_col <- 'cell_type'
```

```{python ppackages}
import anndata as ad
import scanpy as sc
import pandas as pd
import numpy as np
import subprocess
import os
```


# Loading input data {.tabset}

The number of splicing junctions was counted based on cellranger-aligned reads mapped to the following
genes:

```{r genes_tested}
print(read_yaml(snakemake_config)[['genes']])
```

These counts were compiled into a junction-by-barcode matrix for differential testing.


```{python import_adata}

# --------------------------------------------------------------------------------
# This chunk imports input data and add the junction matrix to AnnData
# --------------------------------------------------------------------------------

# reticulate::repl_python()

# Import AnnData objects
adata_dic = {celltype: ad.read_h5ad(path) for celltype, path in r.adata_paths.items()}
adata_filtered = {}

# Import junction matrix and metadata
counts = pd.read_csv(r.mtx_path, sep=" ")

for celltype in adata_dic.keys():

    # Retrieve an AnnData corresponding to the celltype
    ann = adata_dic[celltype]
    # Extract obj metadata
    obs = ann.obs

    # Add new columns required to subset barcodes if necessary
    if r.subset_col is not None:
        for key, value in r.subset_col.items():
            obs[key] = obs[value].astype(str).agg("_".join, axis=1)

    # Replace the input metadata with updated one
    ann.obs = obs
    # Add raw and log1p-transformed junction counts to the AnnData
    ann.obsm['junction_raw'] = counts[obs.index].T
    ann.obsm['junction_lognorm'] = np.log1p(counts[obs.index].T)
    # Update the input AnnData
    adata_dic[celltype] = ann

    # Extract cell barcodes
    target_cells = list(ann.obs.index)
    # Slice the junction count matrix based on the extracted barcodes
    counts_subset = counts[target_cells]
    # Subset metadata
    obs_subset = obs.loc[counts_subset.columns, :]

    # Create a new AnnData obj
    ann_new = ad.AnnData(X=counts_subset.T, obs=obs_subset)
    ann_new.layers['counts'] = ann_new.X.copy()
    # Add log1p-transformed values to the AnnData
    sc.pp.log1p(ann_new)
    ann_new.layers['lognorm'] = ann_new.X.copy()

    # Specify paths to updated/created AnnData
    updated_path = f"{r.out_dir}/{celltype}_JUNCTIONS_ADDED.h5ad"
    new_path = f"{r.out_dir}/{celltype}_JUNCTIONS_ONLY.h5ad"
    # Save updated/created AnnData
    ann.write_h5ad(updated_path)
    ann_new.write_h5ad(new_path)

    # Replace the input AnnData with updated obj
    adata_filtered[celltype] = ann_new

```

Input AnnData objects for each celltype:

```{r infile_info}

for (name in names(adata_paths)) {
    print(paste0(name, ": ", adata_paths[[name]]))
}

```

These input objects have been updated to include the following data:

- `AnnData.obsm["junction_raw"]`: raw junction counts
- `AnnData.obsm["junctions_lognorm"]`: log-normalized junction counts (log1p transformation)

Refer to the following file paths for the updated `AnnData` objects:

```{r outann_info1}
for (name in names(adata_paths)) {
    print(paste0(name, ": ", out_dir, "/", name, "_JUNCTIONS_ADDED.h5ad"))
}
```

In the meantime, new `AnnData` objects have been saved as listed below:

- `AnnData.layers["counts"]`: raw junction counts
- `AnnData.layers["lognorm"]`: log-normalized junction counts (log1p transformation)

The newly created `AnnData` objects are saved at:

```{r outann_info2}
for (name in names(adata_paths)) {
    print(paste0(name, ": ", out_dir, "/", name, "_JUNCTIONS_ONLY.h5ad"))
}
```

Note that the `*_JUNCTIONS_ADDED.h5ad` file includes both the transcript and junction counts, while
the `*_JUNCTIONS_ONLY.h5ad` contains only junction counts. 

The following table shows the number of samples and barcodes for each study- and celltype-specific subset.
The `N_samples` and `N_barcodes` columns represent the number of samples and barcodes 
that were initially loaded, respectively.

```{r n_barcodes, results='asis'}

# --------------------------------------------------------------------------------
# This chunk displays the number of input barcodes for each subset
# --------------------------------------------------------------------------------

# Function to extract the number of samples from a list of AnnData obj
count_n_samples <- function(obj_dic) {
    sapply(obj_dic, function(obj) obj$obs[[sample_col]]) %>%
        unlist() %>%
        unique() %>%
        length()
}

# Prep an data frame exploring the number of samples and barcodes for all cells
n_df <- data.frame(subset="all",
                   N_samples=count_n_samples(py$adata_dic),
                   N_barcodes=sapply(py$adata_dic, function(o) nrow(o$obs)) %>% sum
)

# Add rows for each cell type in all datasets
df <- lapply(py$adata_dic, function(o) as.data.frame(o$obs)) %>%
    bind_rows()
ncells <- split(df, df[[celltype_col]]) %>% 
    sapply(nrow)
nsamples <- split(df, df[[celltype_col]]) %>%
    map_dbl(~.x[, c(celltype_col, sample_col)] %>%
        unique() %>%
        nrow())

n_df <- rbind(n_df,
              data.frame(subset=paste0("all_", names(ncells)),
                         N_samples=nsamples,
                         N_barcodes=ncells) %>%
              remove_rownames()
)

# Add rows for dataset-specific celltypes
for (o in py$adata_dic) {
    df <- as.data.frame(o$obs)
    n_list <- split(df, df[[names(subset_col)[1]]]) %>%
        lapply(nrow)
    n_df <- rbind(n_df,
                  data.frame(subset=names(n_list), 
                             N_samples=count_n_samples(list(o)),
                             N_barcodes=unlist(n_list)))
}
n_df <- n_df %>% remove_rownames()

knitr::kable(n_df)
```

```{r prep_count_list}

# --------------------------------------------------------------------------------
# This chunk concatenates metadata across datasets
# --------------------------------------------------------------------------------

# Concatenate metadata for all barcodes
all_meta <- lapply(py$adata_dic, function(o) o$obs %>% as.data.frame()) %>%
    bind_rows()

# Reorder rows to align with the count matrix
all_meta <- all_meta[colnames(py$counts),]
```

```{python prep_subset_obj}

# --------------------------------------------------------------------------------
# This chunk is used to manually subset AnnData objects
# --------------------------------------------------------------------------------

# reticulate::repl_python()

# Function to normalize and reducing dimensions on an AnnData obj
def preprocess_adata(obj):
    # Normalize counts
    obj.layers['counts'] = obj.X.copy()
    sc.pp.log1p(obj)
    obj.layers['lognorm'] = obj.X.copy()
    # Run PCA
    sc.tl.pca(obj)
    # Find neighbors
    sc.pp.neighbors(obj)
    # Run UMAP
    sc.tl.umap(obj)

    return obj

# Initialize a new dictionary for subsetted AnnData
subadata_dic = {'all': preprocess_adata(ad.AnnData(X=counts.T, obs=r.all_meta))}

subset_col = list(r.subset_col.keys())[0]
celltype_col = r.celltype_col

for subs in r.n_df['subset'][1:]:
    # Specify key column
    if "all" in subs:
        key_col = celltype_col
        target = subs.split("_")[1]
    else:
        key_col = subset_col
        target = subs

    # Retrieve metadata for the subset
    obs_x = r.all_meta[r.all_meta[key_col] == target]
    # Prep the subsetted count matrix
    mat_x = counts[obs_x.index]
    # Create an AnnData obj
    adata_x = ad.AnnData(X=mat_x.T, obs=obs_x)
    # Add log-normalized and dim-reduced AnnData to the dictionary
    subadata_dic[subs] = preprocess_adata(adata_x)


```

```{r aggr_counts}

# --------------------------------------------------------------------------------
# This chunk prepares a list of matrices with columns for barcodes or samples
# --------------------------------------------------------------------------------

# Create 
sc_list <- list()
sp_list <- list()

for (name in names(py$subadata_dic)) {

    # Retrieve an AnnData obj
    o <- py$subadata_dic[[name]]

    # Prep matrices for raw and normalized junction counts per barcode
    mat_list <- list(raw=o$layers[['counts']], norm=o$layers[['lognorm']]) %>%
        lapply(t) %>%
        lapply(function(m) {
            colnames(m) <- rownames(o$obs)
            rownames(m) <-o$var_names$values
            return(m)
        })
    sc_list[[name]] <- mat_list

    # Collapse the single cell matrix to the following dimension: junction-by-sample
    mat <- mat_list[['raw']] %>%
        as.data.frame() %>%
        rownames_to_column('junction') %>%
        pivot_longer(!junction, names_to='barcodes', values_to='junction_counts') %>%
        left_join(o$obs %>% rownames_to_column('barcodes'), by="barcodes") %>%
        as.data.frame() %>%
        unite(col="SampleID_celltype",
              sample_col,
              celltype_col,
              sep="_",
              remove=FALSE) %>%
        dplyr::select(junction, barcodes,
                      junction_counts,
                      SampleID_celltype) %>%
        group_by(junction, SampleID_celltype) %>%
        summarize(counts=sum(junction_counts)) %>%
        spread(SampleID_celltype, counts) %>%
        column_to_rownames('junction')

    # Break if missing values have been introduced to the collapsed matrix
    if (any(is.na(mat))) { paste0(name, ": Missing values introdueced!") }

    sp_list[[name]] <- mat
}

```

# Quality Control (QC)

## Proportion of cells with junctions

The number of barcodes is counted based on whether junctions were detected in each cell. Refer to 
the following columns for the overview:

- `N_barcodes`: all input barcodes
- `N_barcodes_junctions`: barcodes in which any junctions were detected
- `percentage`: `N_barcodes_junctions / N_barcodes x 100`, representing the proportion
  of barcodes with junction detection

```{r n_barcodes_junctions, results='asis'}

# --------------------------------------------------------------------------------
# This chunk displays the number of barcodes with and without junction detection
# --------------------------------------------------------------------------------

n_df <- n_df %>%
    dplyr::select(-N_samples) %>%
    mutate(N_barcodes_junctions=sapply(py$subadata_dic,
                                       function(o) sum(rowSums(o$X) > 0)),
           percentage=N_barcodes_junctions/N_barcodes * 100,
           percentage=round(percentage, 2))

knitr::kable(n_df)
```

## Heatmap {.tabset}

Cell-to-cell similarity in junction profiling is explored using a heatmap generated from non-zero cells, 
with *Euclidean distance* represented by the color scale. Darker blue indicates less distance 
(i.e., greater similarity) in splice junction profiling.

```{r qc_heatmap, results='asis', fig.height=12, fig.width=12}

# --------------------------------------------------------------------------------
# This chunk creates cell-to-cell similarity heatmap
# --------------------------------------------------------------------------------

for (name in names(sp_list)) {
    cat('###', name, '{.tabset}\n\n')
    # Retrieve the raw count matrix
    scmat <- sc_list[[name]][['raw']]
    # Remove zero-count barcodes
    scmat <- scmat[, colSums(scmat) > 0]
    # Transform to a distance matrix after applying log2 transformation
    # with a pseudocount of 1
    scmat <- as.matrix(dist(t(log2(scmat + 1))))

    # Set color to be displayed
    colors <- colorRampPalette(rev(RColorBrewer::brewer.pal(9, 'Blues')))(255)

    # Prep metadata for heatmap
    heatmap_meta <- as.data.frame(py$subadata_dic[[name]][['obs']]) %>%
        unite(col="SampleID_celltype",
              sample_col,
              celltype_col,
              sep="_",
              remove=FALSE) 
    condition_cols <- names(factor_groups)[names(factor_groups) %in% colnames(heatmap_meta)]
    heatmap_meta <- heatmap_meta[colnames(scmat), condition_cols]

    # Print heatmap
    print(pheatmap::pheatmap(
        scmat,
        scale='none',
        color=colors,
        annotation_col=heatmap_meta,
        show_rownames=FALSE,
        show_colnames=FALSE,
        cluster_cols=TRUE,
        border_color=NA
    )
    )
    cat('\n\n')
}
```

## PCA {.tabset}

Another way of looking at sample clustering is principal components analysis (PCA). Each axis does 
not have units, rather, it represents the dimensions along which the samples vary the most. 
The amount of variance explained by each principal component is indicated in the axes label.

```{r pca, results='asis', eval=FALSE}

# --------------------------------------------------------------------------------
# This chunk creates PCA plots across the  cells
# --------------------------------------------------------------------------------

for (name in names(sp_list)) {
    cat('###', name, '{.tabset}\n\n')
    # Retrieve the raw count matrix
    scmat <- sc_list[[name]][['raw']]
    # Remove zero-count barcodes and apply log2-transformation
    scmat <- t(log2(as.matrix(scmat[, colSums(scmat) > 0] + 1)))
    # Run PCA
    pca <- prcomp(scmat, center=TRUE, scale=FALSE)

```

# Saving pseudobulk count matrices

Pseudobulk junction count matrices are saved in the following location:

```{r save_aggr_matrices}

# --------------------------------------------------------------------------------
# This chunk saves single-cell and pseudobulk count matrices for further analysis
# --------------------------------------------------------------------------------

rds <- list(sc=sc_list, bulk=sp_list)
rds_path <- file.path(out_dir, "aggr_matrices.rds")
saveRDS(rds, rds_path)

print(paste0("Path: ", rds_path))
print(paste0("Structure: "))
map(rds, summary)
```

# Session info

```{r session_info, collapse=FALSE}
sessionInfo()
```



